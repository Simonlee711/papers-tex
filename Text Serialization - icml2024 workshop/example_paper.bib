@inproceedings{hegselmann2023tabllm,
  title={Tabllm: Few-shot classification of tabular data with large language models},
  author={Hegselmann, Stefan and Buendia, Alejandro and Lang, Hunter and Agrawal, Monica and Jiang, Xiaoyi and Sontag, David},
  booktitle={International Conference on Artificial Intelligence and Statistics},
  pages={5549--5581},
  year={2023},
  organization={PMLR}
}

@article{vaswani2017attention,
  title={Attention is all you need},
  author={Vaswani, Ashish and Shazeer, Noam and Parmar, Niki and Uszkoreit, Jakob and Jones, Llion and Gomez, Aidan N and Kaiser, {\L}ukasz and Polosukhin, Illia},
  journal={Advances in neural information processing systems},
  volume={30},
  year={2017}
}

@article{bahdanau2014neural,
  title={Neural machine translation by jointly learning to align and translate},
  author={Bahdanau, Dzmitry and Cho, Kyunghyun and Bengio, Yoshua},
  journal={arXiv preprint arXiv:1409.0473},
  year={2014}
}

@article{radford2018improving,
  title={Improving language understanding by generative pre-training},
  author={Radford, Alec and Narasimhan, Karthik and Salimans, Tim and Sutskever, Ilya and others},
  year={2018},
  publisher={OpenAI}
}

@article{devlin2018bert,
  title={Bert: Pre-training of deep bidirectional transformers for language understanding},
  author={Devlin, Jacob and Chang, Ming-Wei and Lee, Kenton and Toutanova, Kristina},
  journal={arXiv preprint arXiv:1810.04805},
  year={2018}
}

@article{lewis2019bart,
  title={Bart: Denoising sequence-to-sequence pre-training for natural language generation, translation, and comprehension},
  author={Lewis, Mike and Liu, Yinhan and Goyal, Naman and Ghazvininejad, Marjan and Mohamed, Abdelrahman and Levy, Omer and Stoyanov, Ves and Zettlemoyer, Luke},
  journal={arXiv preprint arXiv:1910.13461},
  year={2019}
}

@article{grinsztajn2022tree,
  title={Why do tree-based models still outperform deep learning on typical tabular data?},
  author={Grinsztajn, L{\'e}o and Oyallon, Edouard and Varoquaux, Ga{\"e}l},
  journal={Advances in neural information processing systems},
  volume={35},
  pages={507--520},
  year={2022}
}

@article{weiss2016survey,
  title={A survey of transfer learning},
  author={Weiss, Karl and Khoshgoftaar, Taghi M and Wang, DingDing},
  journal={Journal of Big data},
  volume={3},
  pages={1--40},
  year={2016},
  publisher={Springer}
}

@article{radford2019language,
  title={Language models are unsupervised multitask learners},
  author={Radford, Alec and Wu, Jeffrey and Child, Rewon and Luan, David and Amodei, Dario and Sutskever, Ilya and others},
  journal={OpenAI blog},
  volume={1},
  number={8},
  pages={9},
  year={2019}
}

@inproceedings{su2019generalizing,
  title={Generalizing question answering system with pre-trained language model fine-tuning},
  author={Su, Dan and Xu, Yan and Winata, Genta Indra and Xu, Peng and Kim, Hyeondey and Liu, Zihan and Fung, Pascale},
  booktitle={Proceedings of the 2nd workshop on machine reading for question answering},
  pages={203--211},
  year={2019}
}

@article{trinh2024solving,
  title={Solving olympiad geometry without human demonstrations},
  author={Trinh, Trieu H and Wu, Yuhuai and Le, Quoc V and He, He and Luong, Thang},
  journal={Nature},
  volume={625},
  number={7995},
  pages={476--482},
  year={2024},
  publisher={Nature Publishing Group}
}

@article{wang2023mathcoder,
  title={Mathcoder: Seamless code integration in llms for enhanced mathematical reasoning},
  author={Wang, Ke and Ren, Houxing and Zhou, Aojun and Lu, Zimu and Luo, Sichun and Shi, Weikang and Zhang, Renrui and Song, Linqi and Zhan, Mingjie and Li, Hongsheng},
  journal={arXiv preprint arXiv:2310.03731},
  year={2023}
}

@article{imani2023mathprompter,
  title={Mathprompter: Mathematical reasoning using large language models},
  author={Imani, Shima and Du, Liang and Shrivastava, Harsh},
  journal={arXiv preprint arXiv:2303.05398},
  year={2023}
}

@article{dinh2022lift,
  title={Lift: Language-interfaced fine-tuning for non-language machine learning tasks},
  author={Dinh, Tuan and Zeng, Yuchen and Zhang, Ruisu and Lin, Ziqian and Gira, Michael and Rajput, Shashank and Sohn, Jy-yong and Papailiopoulos, Dimitris and Lee, Kangwook},
  journal={Advances in Neural Information Processing Systems},
  volume={35},
  pages={11763--11784},
  year={2022}
}

@misc{asuncion2007uci,
  title={UCI machine learning repository},
  author={Asuncion, Arthur and Newman, David},
  year={2007},
  publisher={Irvine, CA, USA}
}

@article{chen2024multimodal,
  title={Multimodal Clinical Benchmark for Emergency Care (MC-BEC): A Comprehensive Benchmark for Evaluating Foundation Models in Emergency Medicine},
  author={Chen, Emma and Kansal, Aman and Chen, Julie and Jin, Boyang Tom and Reisler, Julia and Kim, David E and Rajpurkar, Pranav},
  journal={Advances in Neural Information Processing Systems},
  volume={36},
  year={2024}
}

@article{kim2024health,
  title={Health-llm: Large language models for health prediction via wearable sensor data},
  author={Kim, Yubin and Xu, Xuhai and McDuff, Daniel and Breazeal, Cynthia and Park, Hae Won},
  journal={arXiv preprint arXiv:2401.06866},
  year={2024}
}

@article{hegselmann2024data,
  title={A Data-Centric Approach To Generate Faithful and High Quality Patient Summaries with Large Language Models},
  author={Hegselmann, Stefan and Shen, Shannon Zejiang and Gierse, Florian and Agrawal, Monica and Sontag, David and Jiang, Xiaoyi},
  journal={arXiv preprint arXiv:2402.15422},
  year={2024}
}

@article{lee2024multimodal,
  title={Multimodal Clinical Pseudo-notes for Emergency Department Prediction Tasks using Multiple Embedding Model for EHR (MEME)},
  author={Lee, Simon A and Jain, Sujay and Chen, Alex and Biswas, Arabdha and Fang, Jennifer and Rudas, Akos and Chiang, Jeffrey N},
  journal={arXiv preprint arXiv:2402.00160},
  year={2024}
}

@inproceedings{belyaeva2023multimodal,
  title={Multimodal llms for health grounded in individual-specific data},
  author={Belyaeva, Anastasiya and Cosentino, Justin and Hormozdiari, Farhad and Eswaran, Krish and Shetty, Shravya and Corrado, Greg and Carroll, Andrew and McLean, Cory Y and Furlotte, Nicholas A},
  booktitle={Workshop on Machine Learning for Multimodal Healthcare Data},
  pages={86--102},
  year={2023},
  organization={Springer}
}

@article{min2024exploring,
  title={Exploring the Impact of Table-to-Text Methods on Augmenting LLM-based Question Answering with Domain Hybrid Data},
  author={Min, Dehai and Hu, Nan and Jin, Rihui and Lin, Nuo and Chen, Jiaoyan and Chen, Yongrui and Li, Yu and Qi, Guilin and Li, Yun and Li, Nijun and others},
  journal={arXiv preprint arXiv:2402.12869},
  year={2024}
}

@article{gidroltext,
  title={Text classification with LLMs},
  author={GIDROL, Jean-Baptiste}
}

@inproceedings{sui2024table,
  title={Table meets llm: Can large language models understand structured table data? a benchmark and empirical study},
  author={Sui, Yuan and Zhou, Mengyu and Zhou, Mingjie and Han, Shi and Zhang, Dongmei},
  booktitle={Proceedings of the 17th ACM International Conference on Web Search and Data Mining},
  pages={645--654},
  year={2024}
}

@article{li2024can,
  title={Can llm already serve as a database interface? a big bench for large-scale database grounded text-to-sqls},
  author={Li, Jinyang and Hui, Binyuan and Qu, Ge and Yang, Jiaxi and Li, Binhua and Li, Bowen and Wang, Bailin and Qin, Bowen and Geng, Ruiying and Huo, Nan and others},
  journal={Advances in Neural Information Processing Systems},
  volume={36},
  year={2024}
}

@article{zhang2018analyzing,
  title={Analyzing complex single-molecule emission patterns with deep learning},
  author={Zhang, Peiyi and Liu, Sheng and Chaurasia, Abhishek and Ma, Donghan and Mlodzianoski, Michael J and Culurciello, Eugenio and Huang, Fang},
  journal={Nature methods},
  volume={15},
  number={11},
  pages={913--916},
  year={2018},
  publisher={Nature Publishing Group US New York}
}

@article{feng2019fringe,
  title={Fringe pattern analysis using deep learning},
  author={Feng, Shijie and Chen, Qian and Gu, Guohua and Tao, Tianyang and Zhang, Liang and Hu, Yan and Yin, Wei and Zuo, Chao},
  journal={Advanced photonics},
  volume={1},
  number={2},
  pages={025001--025001},
  year={2019},
  publisher={Society of Photo-Optical Instrumentation Engineers}
}

@incollection{torrey2010transfer,
  title={Transfer learning},
  author={Torrey, Lisa and Shavlik, Jude},
  booktitle={Handbook of research on machine learning applications and trends: algorithms, methods, and techniques},
  pages={242--264},
  year={2010},
  publisher={IGI global}
}

@article{zhuang2020comprehensive,
  title={A comprehensive survey on transfer learning},
  author={Zhuang, Fuzhen and Qi, Zhiyuan and Duan, Keyu and Xi, Dongbo and Zhu, Yongchun and Zhu, Hengshu and Xiong, Hui and He, Qing},
  journal={Proceedings of the IEEE},
  volume={109},
  number={1},
  pages={43--76},
  year={2020},
  publisher={IEEE}
}

@article{pan2009survey,
  title={A survey on transfer learning},
  author={Pan, Sinno Jialin and Yang, Qiang},
  journal={IEEE Transactions on knowledge and data engineering},
  volume={22},
  number={10},
  pages={1345--1359},
  year={2009},
  publisher={IEEE}
}

@article{niu2020decade,
  title={A decade survey of transfer learning (2010--2020)},
  author={Niu, Shuteng and Liu, Yongxin and Wang, Jian and Song, Houbing},
  journal={IEEE Transactions on Artificial Intelligence},
  volume={1},
  number={2},
  pages={151--166},
  year={2020},
  publisher={IEEE}
}

@article{mayer2020scalable,
  title={Scalable deep learning on distributed infrastructures: Challenges, techniques, and tools},
  author={Mayer, Ruben and Jacobsen, Hans-Arno},
  journal={ACM Computing Surveys (CSUR)},
  volume={53},
  number={1},
  pages={1--37},
  year={2020},
  publisher={ACM New York, NY, USA}
}

@inproceedings{chilimbi2014project,
  title={Project adam: Building an efficient and scalable deep learning training system},
  author={Chilimbi, Trishul and Suzue, Yutaka and Apacible, Johnson and Kalyanaraman, Karthik},
  booktitle={11th USENIX symposium on operating systems design and implementation (OSDI 14)},
  pages={571--582},
  year={2014}
}

@inproceedings{rouhani2018deepsecure,
  title={Deepsecure: Scalable provably-secure deep learning},
  author={Rouhani, Bita Darvish and Riazi, M Sadegh and Koushanfar, Farinaz},
  booktitle={Proceedings of the 55th annual design automation conference},
  pages={1--6},
  year={2018}
}

@article{brown2020language,
  title={Language models are few-shot learners},
  author={Brown, Tom and Mann, Benjamin and Ryder, Nick and Subbiah, Melanie and Kaplan, Jared D and Dhariwal, Prafulla and Neelakantan, Arvind and Shyam, Pranav and Sastry, Girish and Askell, Amanda and others},
  journal={Advances in neural information processing systems},
  volume={33},
  pages={1877--1901},
  year={2020}
}

@article{gorishniy2022embeddings,
  title={On embeddings for numerical features in tabular deep learning},
  author={Gorishniy, Yury and Rubachev, Ivan and Babenko, Artem},
  journal={Advances in Neural Information Processing Systems},
  volume={35},
  pages={24991--25004},
  year={2022}
}

@article{golkar2023xval,
  title={xval: A continuous number encoding for large language models},
  author={Golkar, Siavash and Pettee, Mariel and Eickenberg, Michael and Bietti, Alberto and Cranmer, Miles and Krawezik, Geraud and Lanusse, Francois and McCabe, Michael and Ohana, Ruben and Parker, Liam and others},
  journal={arXiv preprint arXiv:2310.02989},
  year={2023}
}

@inproceedings{chen2016xgboost,
  title={Xgboost: A scalable tree boosting system},
  author={Chen, Tianqi and Guestrin, Carlos},
  booktitle={Proceedings of the 22nd acm sigkdd international conference on knowledge discovery and data mining},
  pages={785--794},
  year={2016}
}

@article{ke2017lightgbm,
  title={Lightgbm: A highly efficient gradient boosting decision tree},
  author={Ke, Guolin and Meng, Qi and Finley, Thomas and Wang, Taifeng and Chen, Wei and Ma, Weidong and Ye, Qiwei and Liu, Tie-Yan},
  journal={Advances in neural information processing systems},
  volume={30},
  year={2017}
}

@article{shwartz2022tabular,
  title={Tabular data: Deep learning is not all you need},
  author={Shwartz-Ziv, Ravid and Armon, Amitai},
  journal={Information Fusion},
  volume={81},
  pages={84--90},
  year={2022},
  publisher={Elsevier}
}

@article{borisov2022deep,
  title={Deep neural networks and tabular data: A survey},
  author={Borisov, Vadim and Leemann, Tobias and Se{\ss}ler, Kathrin and Haug, Johannes and Pawelczyk, Martin and Kasneci, Gjergji},
  journal={IEEE Transactions on Neural Networks and Learning Systems},
  year={2022},
  publisher={IEEE}
}

@article{gorishniy2021revisiting,
  title={Revisiting deep learning models for tabular data},
  author={Gorishniy, Yury and Rubachev, Ivan and Khrulkov, Valentin and Babenko, Artem},
  journal={Advances in Neural Information Processing Systems},
  volume={34},
  pages={18932--18943},
  year={2021}
}

@inproceedings{arik2021tabnet,
  title={Tabnet: Attentive interpretable tabular learning},
  author={Arik, Sercan {\"O} and Pfister, Tomas},
  booktitle={Proceedings of the AAAI conference on artificial intelligence},
  volume={35},
  number={8},
  pages={6679--6687},
  year={2021}
}

@article{hollmann2022tabpfn,
  title={Tabpfn: A transformer that solves small tabular classification problems in a second},
  author={Hollmann, Noah and M{\"u}ller, Samuel and Eggensperger, Katharina and Hutter, Frank},
  journal={arXiv preprint arXiv:2207.01848},
  year={2022}
}

@article{somepalli2021saint,
  title={Saint: Improved neural networks for tabular data via row attention and contrastive pre-training},
  author={Somepalli, Gowthami and Goldblum, Micah and Schwarzschild, Avi and Bruss, C Bayan and Goldstein, Tom},
  journal={arXiv preprint arXiv:2106.01342},
  year={2021}
}

@article{lee2024large,
  title={Do Large Language Models understand Medical Codes?},
  author={Lee, Simon A and Lindsey, Timothy},
  journal={arXiv preprint arXiv:2403.10822},
  year={2024}
}

@inproceedings{ji2023towards,
  title={Towards mitigating LLM hallucination via self reflection},
  author={Ji, Ziwei and Yu, Tiezheng and Xu, Yan and Lee, Nayeon and Ishii, Etsuko and Fung, Pascale},
  booktitle={Findings of the Association for Computational Linguistics: EMNLP 2023},
  pages={1827--1843},
  year={2023}
}

@inproceedings{harari2022few,
  title={Few-shot tabular data enrichment using fine-tuned transformer architectures},
  author={Harari, Asaf and Katz, Gilad},
  booktitle={Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)},
  pages={1577--1591},
  year={2022}
}

@article{huang2020tabtransformer,
  title={Tabtransformer: Tabular data modeling using contextual embeddings},
  author={Huang, Xin and Khetan, Ashish and Cvitkovic, Milan and Karnin, Zohar},
  journal={arXiv preprint arXiv:2012.06678},
  year={2020}
}

@article{kadra2021well,
  title={Well-tuned simple nets excel on tabular datasets},
  author={Kadra, Arlind and Lindauer, Marius and Hutter, Frank and Grabocka, Josif},
  journal={Advances in neural information processing systems},
  volume={34},
  pages={23928--23941},
  year={2021}
}

@article{levin2022transfer,
  title={Transfer learning with deep tabular models},
  author={Levin, Roman and Cherepanova, Valeriia and Schwarzschild, Avi and Bansal, Arpit and Bruss, C Bayan and Goldstein, Tom and Wilson, Andrew Gordon and Goldblum, Micah},
  journal={arXiv preprint arXiv:2206.15306},
  year={2022}
}

@article{liu2022few,
  title={Few-shot parameter-efficient fine-tuning is better and cheaper than in-context learning},
  author={Liu, Haokun and Tam, Derek and Muqeeth, Mohammed and Mohta, Jay and Huang, Tenghao and Bansal, Mohit and Raffel, Colin A},
  journal={Advances in Neural Information Processing Systems},
  volume={35},
  pages={1950--1965},
  year={2022}
}

@article{perez2021true,
  title={True few-shot learning with language models},
  author={Perez, Ethan and Kiela, Douwe and Cho, Kyunghyun},
  journal={Advances in neural information processing systems},
  volume={34},
  pages={11054--11070},
  year={2021}
}

@article{popov2019neural,
  title={Neural oblivious decision ensembles for deep learning on tabular data},
  author={Popov, Sergei and Morozov, Stanislav and Babenko, Artem},
  journal={arXiv preprint arXiv:1909.06312},
  year={2019}
}

@article{sahakyan2021explainable,
  title={Explainable artificial intelligence for tabular data: A survey},
  author={Sahakyan, Maria and Aung, Zeyar and Rahwan, Talal},
  journal={IEEE access},
  volume={9},
  pages={135392--135422},
  year={2021},
  publisher={IEEE}
}

@article{yin2020tabert,
  title={TaBERT: Pretraining for joint understanding of textual and tabular data},
  author={Yin, Pengcheng and Neubig, Graham and Yih, Wen-tau and Riedel, Sebastian},
  journal={arXiv preprint arXiv:2005.08314},
  year={2020}
}

@inproceedings{zhao2021calibrate,
  title={Calibrate before use: Improving few-shot performance of language models},
  author={Zhao, Zihao and Wallace, Eric and Feng, Shi and Klein, Dan and Singh, Sameer},
  booktitle={International conference on machine learning},
  pages={12697--12706},
  year={2021},
  organization={PMLR}
}

@article{sanh2021multitask,
  title={Multitask prompted training enables zero-shot task generalization},
  author={Sanh, Victor and Webson, Albert and Raffel, Colin and Bach, Stephen H and Sutawika, Lintang and Alyafeai, Zaid and Chaffin, Antoine and Stiegler, Arnaud and Scao, Teven Le and Raja, Arun and others},
  journal={arXiv preprint arXiv:2110.08207},
  year={2021}
}

@article{webson2021prompt,
  title={Do prompt-based models really understand the meaning of their prompts?},
  author={Webson, Albert and Pavlick, Ellie},
  journal={arXiv preprint arXiv:2109.01247},
  year={2021}
}

@article{wei2021finetuned,
  title={Finetuned language models are zero-shot learners},
  author={Wei, Jason and Bosma, Maarten and Zhao, Vincent Y and Guu, Kelvin and Yu, Adams Wei and Lester, Brian and Du, Nan and Dai, Andrew M and Le, Quoc V},
  journal={arXiv preprint arXiv:2109.01652},
  year={2021}
}

@article{kojima2022large,
  title={Large language models are zero-shot reasoners},
  author={Kojima, Takeshi and Gu, Shixiang Shane and Reid, Machel and Matsuo, Yutaka and Iwasawa, Yusuke},
  journal={Advances in neural information processing systems},
  volume={35},
  pages={22199--22213},
  year={2022}
}

@article{zhong2021adapting,
  title={Adapting language models for zero-shot learning by meta-tuning on dataset and prompt collections},
  author={Zhong, Ruiqi and Lee, Kristy and Zhang, Zheng and Klein, Dan},
  journal={arXiv preprint arXiv:2104.04670},
  year={2021}
}
@article{song2023restgpt,
  title={Restgpt: Connecting large language models with real-world applications via restful apis},
  author={Song, Yifan and Xiong, Weimin and Zhu, Dawei and Li, Cheng and Wang, Ke and Tian, Ye and Li, Sujian},
  journal={arXiv preprint arXiv:2306.06624},
  year={2023}
}

@article{chen2024beyond,
  title={Beyond Natural Language: LLMs Leveraging Alternative Formats for Enhanced Reasoning and Communication},
  author={Chen, Weize and Yuan, Chenfei and Yuan, Jiarui and Su, Yusheng and Qian, Chen and Yang, Cheng and Xie, Ruobing and Liu, Zhiyuan and Sun, Maosong},
  journal={arXiv preprint arXiv:2402.18439},
  year={2024}
}

@article{yao2023sai,
  title={SAI: Solving AI Tasks with Systematic Artificial Intelligence in Communication Network},
  author={Yao, Lei and Zhang, Yong and Yan, Zilong and Tian, Jialu},
  journal={arXiv preprint arXiv:2310.09049},
  year={2023}
}

@inproceedings{radford2021learning,
  title={Learning transferable visual models from natural language supervision},
  author={Radford, Alec and Kim, Jong Wook and Hallacy, Chris and Ramesh, Aditya and Goh, Gabriel and Agarwal, Sandhini and Sastry, Girish and Askell, Amanda and Mishkin, Pamela and Clark, Jack and others},
  booktitle={International conference on machine learning},
  pages={8748--8763},
  year={2021},
  organization={PMLR}
}

@article{cortes1995support,
  title={Support-vector networks},
  author={Cortes, Corinna and Vapnik, Vladimir},
  journal={Machine learning},
  volume={20},
  pages={273--297},
  year={1995},
  publisher={Springer}
}

@article{breiman2001random,
  title={Random forests},
  author={Breiman, Leo},
  journal={Machine learning},
  volume={45},
  pages={5--32},
  year={2001},
  publisher={Springer}
}

@article{berkson1944application,
  title={Application of the logistic function to bio-assay},
  author={Berkson, Joseph},
  journal={Journal of the American statistical association},
  volume={39},
  number={227},
  pages={357--365},
  year={1944},
  publisher={Taylor \& Francis}
}

@article{chicco2020advantages,
  title={The advantages of the Matthews correlation coefficient (MCC) over F1 score and accuracy in binary classification evaluation},
  author={Chicco, Davide and Jurman, Giuseppe},
  journal={BMC genomics},
  volume={21},
  pages={1--13},
  year={2020},
  publisher={Springer}
}

@article{wolf2019huggingface,
  title={Huggingface's transformers: State-of-the-art natural language processing},
  author={Wolf, Thomas and Debut, Lysandre and Sanh, Victor and Chaumond, Julien and Delangue, Clement and Moi, Anthony and Cistac, Pierric and Rault, Tim and Louf, R{\'e}mi and Funtowicz, Morgan and others},
  journal={arXiv preprint arXiv:1910.03771},
  year={2019}
}

@article{gardner2023tableshift,
  title={Benchmarking Distribution Shift in Tabular Data with TableShift},
  author={Gardner, Josh and Popovic, Zoran and Schmidt, Ludwig},
  journal={Advances in Neural Information Processing Systems},
  year={2023}
}

@inproceedings{smith1988using,
  title={Using the ADAP learning algorithm to forecast the onset of diabetes mellitus},
  author={Smith, Jack W and Everhart, James E and Dickson, WC and Knowler, William C and Johannes, Robert Scott},
  booktitle={Proceedings of the annual symposium on computer application in medical care},
  pages={261},
  year={1988},
  organization={American Medical Informatics Association}
}

@inproceedings{selent2016assistments,
  title={Assistments dataset from multiple randomized controlled experiments},
  author={Selent, Douglas and Patikorn, Thanaporn and Heffernan, Neil},
  booktitle={Proceedings of the Third (2016) ACM Conference on Learning@ Scale},
  pages={181--184},
  year={2016}
}

@inproceedings{dal2015calibrating,
  title={Calibrating probability with undersampling for unbalanced classification},
  author={Dal Pozzolo, Andrea and Caelen, Olivier and Johnson, Reid A and Bontempi, Gianluca},
  booktitle={2015 IEEE symposium series on computational intelligence},
  pages={159--166},
  year={2015},
  organization={IEEE}
}

@article{sanh2019distilbert,
  title={DistilBERT, a distilled version of BERT: smaller, faster, cheaper and lighter},
  author={Sanh, Victor and Debut, Lysandre and Chaumond, Julien and Wolf, Thomas},
  journal={arXiv preprint arXiv:1910.01108},
  year={2019}
}

@article{liu2019roberta,
  title={Roberta: A robustly optimized bert pretraining approach},
  author={Liu, Yinhan and Ott, Myle and Goyal, Naman and Du, Jingfei and Joshi, Mandar and Chen, Danqi and Levy, Omer and Lewis, Mike and Zettlemoyer, Luke and Stoyanov, Veselin},
  journal={arXiv preprint arXiv:1907.11692},
  year={2019}
}

@article{clark2020electra,
  title={Electra: Pre-training text encoders as discriminators rather than generators},
  author={Clark, Kevin and Luong, Minh-Thang and Le, Quoc V and Manning, Christopher D},
  journal={arXiv preprint arXiv:2003.10555},
  year={2020}
}

@article{yang2019xlnet,
  title={Xlnet: Generalized autoregressive pretraining for language understanding},
  author={Yang, Zhilin and Dai, Zihang and Yang, Yiming and Carbonell, Jaime and Salakhutdinov, Russ R and Le, Quoc V},
  journal={Advances in neural information processing systems},
  volume={32},
  year={2019}
}

@article{lan2019albert,
  title={Albert: A lite bert for self-supervised learning of language representations},
  author={Lan, Zhenzhong and Chen, Mingda and Goodman, Sebastian and Gimpel, Kevin and Sharma, Piyush and Soricut, Radu},
  journal={arXiv preprint arXiv:1909.11942},
  year={2019}
}

@article{he2020deberta,
  title={Deberta: Decoding-enhanced bert with disentangled attention},
  author={He, Pengcheng and Liu, Xiaodong and Gao, Jianfeng and Chen, Weizhu},
  journal={arXiv preprint arXiv:2006.03654},
  year={2020}
}

@article{beltagy2020longformer,
  title={Longformer: The long-document transformer},
  author={Beltagy, Iz and Peters, Matthew E and Cohan, Arman},
  journal={arXiv preprint arXiv:2004.05150},
  year={2020}
}

@article{li2023towards,
  title={Towards general text embeddings with multi-stage contrastive learning},
  author={Li, Zehan and Zhang, Xin and Zhang, Yanzhao and Long, Dingkun and Xie, Pengjun and Zhang, Meishan},
  journal={arXiv preprint arXiv:2308.03281},
  year={2023}
}

@article{guyon2003introduction,
  title={An introduction to variable and feature selection},
  author={Guyon, Isabelle and Elisseeff, Andr{\'e}},
  journal={Journal of machine learning research},
  volume={3},
  number={Mar},
  pages={1157--1182},
  year={2003}
}

@article{st1989analysis,
  title={Analysis of variance (ANOVA)},
  author={St, Lars and Wold, Svante and others},
  journal={Chemometrics and intelligent laboratory systems},
  volume={6},
  number={4},
  pages={259--272},
  year={1989},
  publisher={Elsevier}
}

@article{muennighoff2022mteb,
  title={MTEB: Massive text embedding benchmark},
  author={Muennighoff, Niklas and Tazi, Nouamane and Magne, Lo{\"\i}c and Reimers, Nils},
  journal={arXiv preprint arXiv:2210.07316},
  year={2022}
}

@article{yang2024smalltolarge,
  title={SmallToLarge (S2L): Scalable Data Selection for Fine-tuning Large Language Models by Summarizing Training Trajectories of Small Models},
  author={Yang, Yu and Mishra, Siddhartha and Chiang, Jeffrey N and Mirzasoleiman, Baharan},
  journal={arXiv preprint arXiv:2403.07384},
  year={2024}
}

@article{sarkar2022xbnet,
  title={XBNet: An extremely boosted neural network},
  author={Sarkar, Tushar},
  journal={Intelligent Systems with Applications},
  volume={15},
  pages={200097},
  year={2022},
  publisher={Elsevier}
}

@inproceedings{ojha2020multi,
  title={Multi-objective optimisation of multi-output neural trees},
  author={Ojha, Varun and Nicosia, Giuseppe},
  booktitle={2020 IEEE Congress on Evolutionary Computation (CEC)},
  pages={1--8},
  year={2020},
  organization={IEEE}
}

@inproceedings{di2020mutual,
  title={Mutual information maximization in graph neural networks},
  author={Di, Xinhan and Yu, Pengqian and Bu, Rui and Sun, Mingchao},
  booktitle={2020 International Joint Conference on Neural Networks (IJCNN)},
  pages={1--7},
  year={2020},
  organization={IEEE}
}

@article{piech2015deep,
  title={Deep knowledge tracing},
  author={Piech, Chris and Bassen, Jonathan and Huang, Jonathan and Ganguli, Surya and Sahami, Mehran and Guibas, Leonidas J and Sohl-Dickstein, Jascha},
  journal={Advances in neural information processing systems},
  volume={28},
  year={2015}
}

@article{xu2023deep,
  title={Deep isolation forest for anomaly detection},
  author={Xu, Hongzuo and Pang, Guansong and Wang, Yijie and Wang, Yongjun},
  journal={IEEE Transactions on Knowledge and Data Engineering},
  year={2023},
  publisher={IEEE}
}

@misc{misc_iris_53,
  author       = {Fisher,R. A.},
  title        = {{Iris}},
  year         = {1988},
  howpublished = {UCI Machine Learning Repository},
  note         = {{DOI}: https://doi.org/10.24432/C56C76}
}

@misc{misc_wine_109,
  author       = {Aeberhard,Stefan and Forina,M.},
  title        = {{Wine}},
  year         = {1991},
  howpublished = {UCI Machine Learning Repository},
  note         = {{DOI}: https://doi.org/10.24432/C5PC7J}
}

@book{eaton1995titanic,
  title={Titanic: Triumph and tragedy},
  author={Eaton, John P and Haas, Charles},
  year={1995},
  publisher={WW Norton \& Company}
}

@article{dal2014learned,
  title={Learned lessons in credit card fraud detection from a practitioner perspective},
  author={Dal Pozzolo, Andrea and Caelen, Olivier and Le Borgne, Yann-Ael and Waterschoot, Serge and Bontempi, Gianluca},
  journal={Expert systems with applications},
  volume={41},
  number={10},
  pages={4915--4928},
  year={2014},
  publisher={Elsevier}
}

@article{dal2017credit,
  title={Credit card fraud detection: a realistic modeling and a novel learning strategy},
  author={Dal Pozzolo, Andrea and Boracchi, Giacomo and Caelen, Olivier and Alippi, Cesare and Bontempi, Gianluca},
  journal={IEEE transactions on neural networks and learning systems},
  volume={29},
  number={8},
  pages={3784--3797},
  year={2017},
  publisher={IEEE}
}

@misc{sf-crime,
    author = {Wendy Kan},
    title = {San Francisco Crime Classification},
    publisher = {Kaggle},
    year = {2015},
    url = {https://kaggle.com/competitions/sf-crime}
}

@misc{misc_gene_expression_cancer_rna-seq_401,
  author       = {Fiorini,Samuele},
  title        = {{gene expression cancer RNA-Seq}},
  year         = {2016},
  howpublished = {UCI Machine Learning Repository},
  note         = {{DOI}: https://doi.org/10.24432/C5R88H}
}

@article{brown2018heloc,
  title={Heloc applicant risk performance evaluation by topological hierarchical decomposition},
  author={Brown, Kyle and Doran, Derek and Kramer, Ryan and Reynolds, Brad},
  journal={arXiv preprint arXiv:1811.10658},
  year={2018}
}

@article{kaplan2020scaling,
  title={Scaling laws for neural language models},
  author={Kaplan, Jared and McCandlish, Sam and Henighan, Tom and Brown, Tom B and Chess, Benjamin and Child, Rewon and Gray, Scott and Radford, Alec and Wu, Jeffrey and Amodei, Dario},
  journal={arXiv preprint arXiv:2001.08361},
  year={2020}
}

@misc{lee2024emergency,
      title={Emergency Department Decision Support using Clinical Pseudo-notes}, 
      author={Simon A. Lee and Sujay Jain and Alex Chen and Kyoka Ono and Jennifer Fang and Akos Rudas and Jeffrey N. Chiang},
      year={2024},
      eprint={2402.00160},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}

@misc{fang2024large,
      title={Large Language Models(LLMs) on Tabular Data: Prediction, Generation, and Understanding -- A Survey}, 
      author={Xi Fang and Weijie Xu and Fiona Anting Tan and Jiani Zhang and Ziqing Hu and Yanjun Qi and Scott Nickleach and Diego Socolinsky and Srinivasan Sengamedu and Christos Faloutsos},
      year={2024},
      eprint={2402.17944},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}

@misc{jaitly2023better,
      title={Towards Better Serialization of Tabular Data for Few-shot Classification with Large Language Models}, 
      author={Sukriti Jaitly and Tanay Shah and Ashish Shugani and Razik Singh Grewal},
      year={2023},
      eprint={2312.12464},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}

@article{zhang2023towards,
  title={Towards foundation models for learning on tabular data},
  author={Zhang, Han and Wen, Xumeng and Zheng, Shun and Xu, Wei and Bian, Jiang},
  journal={arXiv preprint arXiv:2310.07338},
  year={2023}
}